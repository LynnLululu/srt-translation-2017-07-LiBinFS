1
00:00:00,240 --> 00:00:03,450
所以 现在我们已经准备好完成网络爬虫 记住

2
00:00:03,450 --> 00:00:05,600
我们想要网络爬虫做什么 我们有一个

3
00:00:05,600 --> 00:00:08,420
种子页面 我们假设我们知道一些种子页面

4
00:00:08,420 --> 00:00:11,460
可以开始 并且种子页面上有一些链接

5
00:00:12,470 --> 00:00:14,920
我们希望能够找到这些链接 而且我们

6
00:00:14,920 --> 00:00:17,330
知道该如何做到这一点 我们将让它们成为

7
00:00:17,330 --> 00:00:20,670
列表 然后我们将跟随这些链接 所以 我们将跟随

8
00:00:20,670 --> 00:00:25,390
这些链接到新的页面 而这些新的页面可能也有

9
00:00:25,390 --> 00:00:29,180
链接 我们想要跟随这些链接 所以

10
00:00:29,180 --> 00:00:31,560
为了做到这一点 我们需要思考的问题

11
00:00:31,560 --> 00:00:33,240
两件事情 我们需要跟踪所有

12
00:00:33,240 --> 00:00:36,950
我们正在等待抓取的网页 我们将引入

13
00:00:36,950 --> 00:00:40,090
一个变量 tocrawl 做到这一点 而 tocrawl

14
00:00:40,090 --> 00:00:43,840
将是一个待抓取页面的列表

15
00:00:45,420 --> 00:00:48,580
列表最初 将只是种子页面 一旦

16
00:00:48,580 --> 00:00:50,410
我们收集种子页面上的链接

17
00:00:50,410 --> 00:00:54,130
它将也包括这些链接 一旦我们完成抓取

18
00:00:54,130 --> 00:00:56,990
我们就不希望保持它在 tocrawl 里了

19
00:00:56,990 --> 00:00:59,270
当我们发现要抓取的新网页时 它们将被添加到

20
00:00:59,270 --> 00:01:03,280
tocrawl 列表里 我们想要的另一个变量是跟踪

21
00:01:03,280 --> 00:01:06,660
所有我们已抓取的网页 在我们抓取

22
00:01:06,660 --> 00:01:09,400
结束的时候 这就是结果 我们想知道所有

23
00:01:09,400 --> 00:01:11,980
我们发现的网页 这将储存在这个变量中 我们

24
00:01:11,980 --> 00:01:15,480
称之为 crawled 因此 让我们通过一个例子来说明这应该如何

25
00:01:15,480 --> 00:01:17,280
在示例网站上工作

26
00:01:19,890 --> 00:01:26,280
所以 我要使种子页面为 www.udacity.com/cs101x/index.html

27
00:01:26,280 --> 00:01:28,900
也就是这里的这个页面 这意味着 当我们开始

28
00:01:28,900 --> 00:01:31,440
抓取 我们希望 tocrawl 是这个

29
00:01:31,440 --> 00:01:33,990
索引页面 我将停止写出

30
00:01:33,990 --> 00:01:37,060
完整的URL 只是写出最后一部分

31
00:01:37,060 --> 00:01:39,050
因为我们抓取的所有网页都会

32
00:01:39,050 --> 00:01:42,150
在我们的测试网站上 所以 tocrawl 将

33
00:01:42,150 --> 00:01:47,030
是只包含一个元素的列表 index.html 页面

34
00:01:47,030 --> 00:01:49,730
我们尚未抓取任何内容 我们才刚刚起步 所以

35
00:01:49,730 --> 00:01:53,430
crawled 将会初始化为一个空列表 下一个

36
00:01:53,430 --> 00:01:55,040
接下来我们要做的是抓取此页面 所以

37
00:01:55,040 --> 00:01:57,490
我们将获取此页面上的所有链接 这意味着

38
00:01:57,490 --> 00:02:01,510
我们已抓取了索引页面 所以现在它将

39
00:02:01,510 --> 00:02:04,710
加入 crawled 但是 当我们查看

40
00:02:04,710 --> 00:02:07,030
索引页上的链接时 我们在该页发现了三个新的链接

41
00:02:07,030 --> 00:02:12,030
我们在这里找到一个链接：crawling.html

42
00:02:12,030 --> 00:02:15,840
我们在这里发现的链接是 walking.html

43
00:02:15,840 --> 00:02:20,470
然后我们在这里发现的链接是 flying.html

44
00:02:20,470 --> 00:02:23,810
所以 在抓取此页面后 tocrawl 的新值

45
00:02:23,810 --> 00:02:27,070
将具有这三个链接 接下来

46
00:02:27,070 --> 00:02:29,150
我们要做的是拿出其中一个链接

47
00:02:29,150 --> 00:02:32,520
并抓取 这个顺序实际上是相当重要的

48
00:02:32,520 --> 00:02:35,200
在获得良好的抓取方面 让我们假设现在

49
00:02:35,200 --> 00:02:36,970
我们将首先处理最后一个 所以

50
00:02:36,970 --> 00:02:39,880
我们将处理链接 fly 链接到

51
00:02:39,880 --> 00:02:42,930
flying 页面 这里那个页面 所以

52
00:02:42,930 --> 00:02:46,060
我们将要抓取页面 flying.html 这个

53
00:02:46,060 --> 00:02:48,880
页面没有任何链接 如果你不知道

54
00:02:48,880 --> 00:02:51,980
为什么 squeamish ossifrage 是魔法咒语 我

55
00:02:51,980 --> 00:02:55,790
建议你去搜索 DuckDuckGo 或谷歌 和

56
00:02:55,790 --> 00:02:57,890
现在我们已经完成了 flying 页面的抓取 所以这会

57
00:02:57,890 --> 00:03:02,115
被添加到 crawled 列表 该列表目前已经有

58
00:03:02,115 --> 00:03:05,260
index.html 页面 我们不会失去我们将添加

59
00:03:05,260 --> 00:03:07,410
一个新的元素 也就是 flying 页面 到该列表

60
00:03:08,980 --> 00:03:10,980
我们完成了它的抓取 所以我们不希望再次抓取

61
00:03:10,980 --> 00:03:14,410
让我们从 tocrawl 列表中删除它 现在

62
00:03:14,410 --> 00:03:17,800
在我们抓取 flying 页面后 我们还有两个链接

63
00:03:17,800 --> 00:03:20,950
留在我们的 tocrawl 列表中 我们有两个链接

64
00:03:20,950 --> 00:03:23,400
已抓取 所以让我们尝试另一个链接 让我们

65
00:03:23,400 --> 00:03:27,280
假设我们跟踪 crawling.html 链接我们跟着抓取

66
00:03:28,320 --> 00:03:31,150
我们去到了这个页面 为了跟随抓取 我们

67
00:03:31,150 --> 00:03:33,620
我们将遵循与处理 flying 页面相同的算法

68
00:03:33,620 --> 00:03:37,940
因此 从 tocrawl 列表中移除它 将它添加到

69
00:03:37,940 --> 00:03:42,100
crawled 列表 所以我们就完成了抓取 抓取 现在

70
00:03:42,100 --> 00:03:43,870
我们想要在我们的 tocrawl 列表中添加所有

71
00:03:43,870 --> 00:03:46,170
我们在这个页面上发现的链接 好了 我们找到了

72
00:03:46,170 --> 00:03:50,400
这个链接 kicking 指向页面 kicking.html 所以我们要

73
00:03:50,400 --> 00:03:54,260
将其添加到我们的 tocrawl 网页列表中 现在

74
00:03:54,260 --> 00:03:57,640
我们继续 我们要继续前进

75
00:03:57,640 --> 00:04:00,600
我们将跟随 kicking 我们发现它没有

76
00:04:00,600 --> 00:04:02,980
任何链接 然后这将添加 kicking

77
00:04:02,980 --> 00:04:05,678
到 crawled 列表中 并将其在

78
00:04:05,678 --> 00:04:08,090
tocrawl 列表中删除 我们会继续

79
00:04:08,090 --> 00:04:10,380
直到我们有没有更多的要抓取的网页 所以

80
00:04:10,380 --> 00:04:12,750
让我更正式地描述一下这个过程

81
00:04:12,750 --> 00:04:14,190
然后我就会问你一个关于它的问题

