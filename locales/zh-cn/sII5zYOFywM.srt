1
00:00:00,420 --> 00:00:02,520
我们要做的第一件事是

2
00:00:02,520 --> 00:00:05,320
添加所有在正在爬取网页上找到的链接到 tocrawl

3
00:00:05,320 --> 00:00:08,880
而这个网页也就是 page 的值 这里最好的方法是

4
00:00:08,880 --> 00:00:11,970
用我们在之前测验中定义的 union 函数

5
00:00:11,970 --> 00:00:15,370
这样可以避免在 tocrawl 中有重复的值

6
00:00:15,370 --> 00:00:19,375
如果你不用 union 的话 也是没有问题的

7
00:00:19,375 --> 00:00:21,940
因为之前我们添加了测试 避免重复爬取一个页面

8
00:00:21,940 --> 00:00:25,440
所以即使有重复值也是没有问题的 我就直接用 union 了

9
00:00:25,440 --> 00:00:27,910
我们将要把在正在爬取页面上

10
00:00:27,910 --> 00:00:31,160
找到的所有链接并入 tocrawl 中去

11
00:00:31,160 --> 00:00:33,150
首先我们需要把 page 传入 get_page

12
00:00:33,150 --> 00:00:36,060
来获得这个页面的内容 然后

13
00:00:36,060 --> 00:00:39,820
之前定义过的 get_all_links 函数会返回

14
00:00:39,820 --> 00:00:43,090
包含有这个页面上所有链接的列表 之后

15
00:00:43,090 --> 00:00:45,920
我们要记录已经爬取过的页面

16
00:00:45,920 --> 00:00:50,600
可以用 append 来完成

17
00:00:50,600 --> 00:00:52,450
它会把页面添加入已爬取页面列表中去

18
00:00:53,580 --> 00:00:56,390
这样我们就完成了 一个可以真正工作的网络爬虫

19
00:00:56,390 --> 00:00:58,420
对于任何种子页面 它都将找到所有能够

20
00:00:58,420 --> 00:01:01,370
从这个页面到达的页面 同时用列表形式返回
